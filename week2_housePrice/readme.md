# üè† Previs√£o de Pre√ßos de Im√≥veis (Kaggle)

Este reposit√≥rio cont√©m minha solu√ß√£o para a competi√ß√£o do Kaggle **[House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/home-data-for-ml-course)**. O objetivo √© prever o pre√ßo final de venda de casas em Ames, Iowa, com base em 79 features diferentes.

Este projeto foi desenvolvido como parte do curso "Intro to Machine Learning" do Kaggle, cobrindo todo o pipeline de um projeto de ci√™ncia de dados, desde a explora√ß√£o inicial at√© o envio de previs√µes.

## ‚öôÔ∏è Fluxo de Trabalho do Projeto

A an√°lise foi realizada em um Jupyter Notebook (`main.ipynb`) e seguiu as seguintes etapas:

1.  **Carregamento dos Dados:** Leitura dos arquivos `train.csv` e `test.csv`.
2.  **An√°lise Explorat√≥ria e Limpeza:**
    * Identifica√ß√£o da vari√°vel alvo (`SalePrice`).
    * Tratamento de dados ausentes (`NaN`), por exemplo, preenchendo o `LotFrontage` com o valor m√©dio da coluna.
3.  **Engenharia de Features (Pr√©-processamento):**
    * Identifica√ß√£o de colunas categ√≥ricas (ambas do tipo `object` e num√©ricas que representam categorias, como `MSSubClass`).
    * Convers√£o de todas as colunas categ√≥ricas para um formato num√©rico usando **One-Hot Encoding** (`pd.get_dummies()`).
4.  **Alinhamento das Features:**
    * Um passo crucial foi garantir que os dados de treino e teste tivessem exatamente as mesmas colunas ap√≥s o *encoding*. Isso foi feito usando `.reindex()`, o que evitou erros de `feature mismatch` durante a predi√ß√£o.
5.  **Divis√£o dos Dados:** Separa√ß√£o dos dados de treino em conjuntos de `treino` (80%) e `valida√ß√£o` (20%) para avaliar os modelos localmente.

## üöÄ Modelagem e Resultados

Dois modelos de regress√£o baseados em √°rvores foram treinados e comparados. A m√©trica de avalia√ß√£o utilizada foi o **Mean Absolute Error (MAE)**, que representa o erro m√©dio, em d√≥lares, das previs√µes.

| Modelo | MAE no Kaggle (Public Score) |
| :--- | :--- |
| **Random Forest Regressor** | `$23.365` |
| **XGBoost Regressor (Otimizado)** | **`$22.335`** |

### Conclus√µes:

* O modelo **Random Forest** serviu como uma √≥tima *baseline* inicial.
* A migra√ß√£o para o **XGBoost** (Extreme Gradient Boosting), utilizando a fun√ß√£o de `early_stopping` para encontrar o n√∫mero ideal de √°rvores, resultou em uma **redu√ß√£o de mais de $1.000** no erro m√©dio, demonstrando ser um algoritmo mais poderoso para este conjunto de dados.

## üõ†Ô∏è Tecnologias Utilizadas

* Python 3
* Pandas
* Numpy
* Scikit-learn (para `RandomForestRegressor`, `train_test_split`, `mean_absolute_error`)
* XGBoost (para `XGBRegressor`)
* Jupyter Notebook